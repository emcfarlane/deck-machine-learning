---
title: Gradient descent
---
What is gradient descent?
<!--question-->
Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.

$$ 
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)
$$

